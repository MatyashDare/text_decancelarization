{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#:pip install datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>path</th>\n",
       "      <th>original</th>\n",
       "      <th>new</th>\n",
       "      <th>var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>131</td>\n",
       "      <td>0024c525-9f48-4e21-8eec-d00aea56deaa</td>\n",
       "      <td>В целях продвижения нового канала продаж «Цифр...</td>\n",
       "      <td>Цель продвижения  канала продаж «Цифровой офис...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1119</td>\n",
       "      <td>1184</td>\n",
       "      <td>00e3fe8f-114e-4b83-a15f-aa7af38f931a</td>\n",
       "      <td>в разрезе центров прибыли верхнего уровня:</td>\n",
       "      <td>по прибыли верхнего уровня:</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1033</td>\n",
       "      <td>1123</td>\n",
       "      <td>00e19c69-1d75-4459-b4e3-8256a6fbdaf3</td>\n",
       "      <td>в срок до 16.12.2021 включительно сформировать...</td>\n",
       "      <td>до 16.12.2021 определить победителей Акции;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                  path  \\\n",
       "0         118    131  0024c525-9f48-4e21-8eec-d00aea56deaa   \n",
       "1        1119   1184  00e3fe8f-114e-4b83-a15f-aa7af38f931a   \n",
       "2        1033   1123  00e19c69-1d75-4459-b4e3-8256a6fbdaf3   \n",
       "\n",
       "                                            original  \\\n",
       "0  В целях продвижения нового канала продаж «Цифр...   \n",
       "1         в разрезе центров прибыли верхнего уровня:   \n",
       "2  в срок до 16.12.2021 включительно сформировать...   \n",
       "\n",
       "                                                 new  var  \n",
       "0  Цель продвижения  канала продаж «Цифровой офис...    4  \n",
       "1                        по прибыли верхнего уровня:    1  \n",
       "2        до 16.12.2021 определить победителей Акции;    4  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('../data/markup2train.csv')\n",
    "test_df = pd.read_csv('../data/markup2test.csv')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5841, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=train_df[train_df['new'].isna()==False].reset_index()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-27 12:37:44.664787: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the backbone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "backbone_id = \"sberbank-ai/rugpt3large_based_on_gpt2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(backbone_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(backbone_id, pad_token=\"<pad>\", eos_token=\"<pad>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the prompt format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import PromptFormat\n",
    "\n",
    "prompt_format = PromptFormat(\"<P*100>{original}<P*20>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parametrization of trainable embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import TensorPromptProvider\n",
    "from transformers import set_seed\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "prompt_provider = TensorPromptProvider()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compose prompt format and prompt provider into prompt object and apply it to the model and tokenizer, i.e. add special tokens to the tokenizer and modify the layer of input embeddings of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ruprompts import Prompt\n",
    "\n",
    "prompt = Prompt(prompt_format, prompt_provider)\n",
    "prompt.patch(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data:\n",
    "1. format the data entries with the specified prompt format\n",
    "2. tokenize the resulting sequences\n",
    "3. truncate the `truncation_field` if sequence length exceeds `max_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "695714baee4b4284b76421e4d90bc324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5256 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23df4fbdefc04d878893948c97c04479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/585 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ruprompts import Text2TextPreprocessor\n",
    "\n",
    "preprocessor = Text2TextPreprocessor(\n",
    "    prompt_format=prompt_format,\n",
    "    tokenizer=tokenizer,\n",
    "    target_field=\"new\",\n",
    "    truncation_field=\"original\",\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.map(preprocessor)\n",
    "valid_dataset = valid_dataset.map(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"promts_checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    learning_rate=0.1,\n",
    "    num_train_epochs=15,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose optimization options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(prompt_provider.parameters(), lr=training_args.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the callbacks and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):\n",
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 5256\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9855' max='9855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9855/9855 30:47, Epoch 15/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.158400</td>\n",
       "      <td>0.578571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.649600</td>\n",
       "      <td>0.539967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.603800</td>\n",
       "      <td>0.505845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.577900</td>\n",
       "      <td>0.486589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>0.477013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.547500</td>\n",
       "      <td>0.462273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.538400</td>\n",
       "      <td>0.460558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.455532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.453796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.514700</td>\n",
       "      <td>0.447421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.448025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.441326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.497400</td>\n",
       "      <td>0.437891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.434825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.488400</td>\n",
       "      <td>0.434566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-657\n",
      "Configuration saved in promts_checkpoints/checkpoint-657/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-657/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-1314\n",
      "Configuration saved in promts_checkpoints/checkpoint-1314/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-1314/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-1971\n",
      "Configuration saved in promts_checkpoints/checkpoint-1971/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-1971/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-2628\n",
      "Configuration saved in promts_checkpoints/checkpoint-2628/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-2628/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-3285\n",
      "Configuration saved in promts_checkpoints/checkpoint-3285/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-3285/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-3942\n",
      "Configuration saved in promts_checkpoints/checkpoint-3942/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-3942/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-4599\n",
      "Configuration saved in promts_checkpoints/checkpoint-4599/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-4599/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-5256\n",
      "Configuration saved in promts_checkpoints/checkpoint-5256/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-5256/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-5913\n",
      "Configuration saved in promts_checkpoints/checkpoint-5913/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-5913/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-6570\n",
      "Configuration saved in promts_checkpoints/checkpoint-6570/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-6570/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-7227\n",
      "Configuration saved in promts_checkpoints/checkpoint-7227/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-7227/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-7884\n",
      "Configuration saved in promts_checkpoints/checkpoint-7884/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-7884/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-8541\n",
      "Configuration saved in promts_checkpoints/checkpoint-8541/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-8541/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-9198\n",
      "Configuration saved in promts_checkpoints/checkpoint-9198/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-9198/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__. If new, Unnamed: 0, path, var, level_0, original, index, __index_level_0__ are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 585\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to promts_checkpoints/checkpoint-9855\n",
      "Configuration saved in promts_checkpoints/checkpoint-9855/config.json\n",
      "Model weights saved in promts_checkpoints/checkpoint-9855/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9855, training_loss=0.5794558791688467, metrics={'train_runtime': 1848.8938, 'train_samples_per_second': 42.642, 'train_steps_per_second': 5.33, 'total_flos': 1.339784227196928e+16, 'train_loss': 0.5794558791688467, 'epoch': 15.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from ruprompts.callbacks import (\n",
    "    FreezeTransformerUnfreezePrompt,\n",
    "    ReduceCheckpoint,\n",
    "    SavePretrainedPrompt,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=preprocessor.collate_fn(),\n",
    "    optimizers=(optimizer, None),\n",
    "    callbacks=[FreezeTransformerUnfreezePrompt(), ReduceCheckpoint(), SavePretrainedPrompt(prompt)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load prompt from the last checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.max_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "prompt = Prompt.from_pretrained(f\"promts_checkpoints/checkpoint-9855\")\n",
    "\n",
    "ppln = pipeline(\"text2text-generation-with-prompt\", prompt=prompt, model=model, tokenizer=tokenizer, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = ['Начиная с премирования по итогам работы за январь 2021 г., руководствоваться перечнем операций и коэффициентами пересчета продуктов в условные продукты (далее - УП) для менеджеров по продажам в соответствии с Приложением к настоящему Распоряжению.',\n",
    "            'В случае подтверждения устранения Катастрофической или Серьезной ошибки Заказчик самостоятельно осуществляет тиражирование ПО в своих подразделениях и филиалах.',\n",
    "            'Контроль за исполнением настоящего Распоряжения оставляю за собой.',\n",
    "            'Утвердить перечень автоматизированных систем и информационных ресурсов Банка, доступных к подключению Cотрудникам (Приложение 1).']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 12:21:15.991761: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Начиная с премирования по итогам работы за январь 2021 г., руководствоваться перечнем операций и коэффициентами пересчета продуктов в условные продукты (далее - УП) для менеджеров по продажам в соответствии с Приложением к настоящему Распоряжению.\n",
      "REPHRASED:  Начиная с премирования по итогам работы за январь 2021 г., руководствоваться перечнем операций и коэффициентами пересчета продуктов в условные продукты (далее - УП) для менеджеров по продажам по Приложению к этому Распоряжению.\n",
      "\n",
      "####\n",
      "\n",
      "ORIGINAL: В случае подтверждения устранения Катастрофической или Серьезной ошибки Заказчик самостоятельно осуществляет тиражирование ПО в своих подразделениях и филиалах.\n",
      "REPHRASED:  При подтверждении устранения Катастрофической или Серьезной ошибки Заказчик самостоятельно тиражирует ПО в подразделениях и филиалах.\n",
      "\n",
      "####\n",
      "\n",
      "ORIGINAL: Контроль за исполнением настоящего Распоряжения оставляю за собой.\n",
      "REPHRASED:  Исполнение этого Распоряжения оставляю за собой.\n",
      "\n",
      "####\n",
      "\n",
      "ORIGINAL: Утвердить перечень автоматизированных систем и информационных ресурсов Банка, доступных к подключению Cотрудникам (Приложение 1).\n",
      "REPHRASED:  Установить перечень автоматизированных систем и информационных ресурсов Банка, доступных к подключению Cотрудникам (Приложение 1).\n",
      "\n",
      "####\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "beam_count = 10\n",
    "\n",
    "for test_string in test_text:\n",
    "    \n",
    "    options = ppln(\n",
    "        {\"original\": test_string},\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,   # unk\n",
    "        num_beams=beam_count,)\n",
    "    print(f\"ORIGINAL: {test_string}\")\n",
    "    print(\"REPHRASED: \",options[0]['generated_text'])\n",
    "    print(\"\\n####\", end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
